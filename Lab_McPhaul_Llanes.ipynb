{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cammylexi/CS2341-Assignment-3/blob/main/Lab_McPhaul_Llanes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "ExuCmUm6SQMb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s96J9P-YSGly",
        "outputId": "bdd6ebdd-e110-4407-c4a1-aff3781147e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/animal-faces\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"andrewmvd/animal-faces\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For license plate detection, I recommend these metrics:\n",
        "\n",
        "Mean Average Precision (mAP): This is the standard evaluation metric for object detection tasks, calculated across different\n",
        "IoU thresholds.\n",
        "\n",
        "F1-Score: The harmonic mean of precision and recall, providing a balanced measure of detection performance.\n",
        "\n",
        "IoU (Intersection over Union): Directly measures how well the predicted bounding boxes match the ground truth.\n",
        "\n",
        "<br>\n",
        "\n",
        "Justification: In license plate detection for applications like traffic monitoring or parking management, both false positives and false negatives can have significant impacts. False positives might lead to incorrect vehicle identification or unnecessary processing, while false negatives could result in missed vehicles. The mAP metric provides a comprehensive evaluation across different confidence thresholds, while F1-score gives a balanced view of precision and recall. IoU directly measures localization accuracy, which is crucial for downstream tasks like character recognition."
      ],
      "metadata": {
        "id": "sPNWihT00LP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "train_images_path = os.path.join(path, \"afhq\", \"train\")\n",
        "print(f\"Train images path: {train_images_path}\")\n",
        "\n",
        "# Define the mapping from class name to one-hot vector\n",
        "class_names = ['cat', 'dog', 'wild']\n",
        "class_to_onehot = {\n",
        "    name: np.eye(len(class_names))[i] for i, name in enumerate(class_names)\n",
        "}\n",
        "\n",
        "def load_image(image_path, target_size=(224, 224)):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, target_size)\n",
        "    return img\n",
        "\n",
        "def load_all_images_and_labels(root_dir, target_size=(224, 224)):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(root_dir, class_name)\n",
        "        if not os.path.isdir(class_dir):\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(class_dir):\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                image_path = os.path.join(class_dir, file)\n",
        "                img = load_image(image_path, target_size)\n",
        "                images.append(img)\n",
        "                labels.append(class_to_onehot[class_name])\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Example usage\n",
        "all_images, all_labels = load_all_images_and_labels(train_images_path)\n",
        "print(f\"Loaded {len(all_images)} images and {len(all_labels)} labels.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCQiaEe9uqz2",
        "outputId": "afbf5b87-ab64-4ff0-e672-abee1950cfe5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images path: /kaggle/input/animal-faces/afhq/train\n",
            "Loaded 14630 images and 14630 labels.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def create_data_augmentation_model():\n",
        "    \"\"\"\n",
        "    Creates a data augmentation model appropriate for animal face classification.\n",
        "    \"\"\"\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        # Randomly flip images horizontally\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "\n",
        "        # Apply small random rotations (animals can appear at different angles)\n",
        "        layers.RandomRotation(0.1),\n",
        "\n",
        "        # Random brightness adjustments to account for lighting variations\n",
        "        layers.RandomBrightness(factor=0.2),\n",
        "\n",
        "        # Random zoom to simulate different distances/perspectives\n",
        "        layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)),\n",
        "\n",
        "        # Slight shifts in position\n",
        "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "\n",
        "        # Contrast adjustments for different lighting conditions\n",
        "        layers.RandomContrast(factor=0.2),\n",
        "    ])\n",
        "\n",
        "    return data_augmentation\n",
        "\n",
        "# The augmentation can be applied as a layer in the model\n",
        "# Or used during training with a data generator"
      ],
      "metadata": {
        "id": "dT0h4nag1YfJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JgMiLURy1gA8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and validation splits (use a smaller subset for quicker testing)\n",
        "max_images = min(5000, len(image_paths))  # Limit to 5000 images for faster processing\n",
        "selected_paths = image_paths[:max_images]\n",
        "train_paths, val_paths = train_test_split(selected_paths, test_size=0.2, random_state=42)\n",
        "print(f\"Training set: {len(train_paths)} images\")\n",
        "print(f\"Validation set: {len(val_paths)} images\")\n",
        "\n",
        "# Create TensorFlow data pipelines\n",
        "def create_dataset(image_paths, batch_size=32):\n",
        "    # Load a small batch of images and labels for initial testing\n",
        "    max_to_load = min(500, len(image_paths))  # Limit for initial testing\n",
        "    paths_to_load = image_paths[:max_to_load]\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    print(f\"Loading {len(paths_to_load)} images...\")\n",
        "    for i, path in enumerate(paths_to_load):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Loaded {i}/{len(paths_to_load)} images\")\n",
        "        try:\n",
        "            img, label = load_image_and_label(path)\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {path}: {e}\")\n",
        "\n",
        "    print(\"Converting to numpy arrays...\")\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    print(f\"Image array shape: {images.shape}\")\n",
        "    print(f\"Label array shape: {labels.shape}\")\n",
        "\n",
        "    # Create TensorFlow dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset, images, labels\n",
        "\n",
        "# Create datasets\n",
        "print(\"Creating training dataset...\")\n",
        "train_dataset, train_images, train_labels = create_dataset(train_paths)\n",
        "print(\"Creating validation dataset...\")\n",
        "val_dataset, val_images, val_labels = create_dataset(val_paths)"
      ],
      "metadata": {
        "id": "jIl809PwTl7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),  # Small rotations only\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomBrightness(0.2),\n",
        "    layers.RandomContrast(0.2),\n",
        "    layers.RandomTranslation(0.1, 0.1)\n",
        "])\n",
        "\n",
        "# Visualize some augmented images\n",
        "print(\"Visualizing augmented images...\")\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i, (image, label) in enumerate(train_dataset.take(1)):\n",
        "    for j in range(9):\n",
        "        augmented_image = data_augmentation(image)\n",
        "        ax = plt.subplot(3, 3, j + 1)\n",
        "        plt.imshow(augmented_image[0].numpy().astype(\"uint8\"))\n",
        "        plt.title(f\"Confidence: {label[0][0]:.2f}\")\n",
        "        plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Data augmentation visualization complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "SwmP6S4FX_t6",
        "outputId": "106a8e40-eefb-401f-dedf-604f5ed9652d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'keras' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1cdc9d95e473>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m data_augmentation = keras.Sequential([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"horizontal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomRotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Small rotations only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomZoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomBrightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the dataset is already split ???\n",
        "\n",
        "Justification:\n",
        "\n",
        "Stratification: Ensures that each fold has a similar distribution of license plate types, positions, and image conditions. This can be based on metadata like image brightness, license plate size relative to image, etc.\n",
        "\n",
        "K-Fold: Utilizes all available data for both training and validation, which is important when working with complex models that benefit from more training examples.\n",
        "\n",
        "Real-world Simulation: Cross-validation better simulates how the model would perform across different datasets and conditions, providing a more robust estimate of its generalization capabilities."
      ],
      "metadata": {
        "id": "ZlqWZOHBsdhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modeling"
      ],
      "metadata": {
        "id": "KzqwW8dSSX8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exceptional Work"
      ],
      "metadata": {
        "id": "EGpwFSflSaay"
      }
    }
  ]
}